{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nbpresent\n",
    "nbpresent.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#pre-req\n",
    "#Install: Anaconda 4.4.0, Keras 2.0.6, Tensorflow 1.2.1, Python 3.6.1, RISE, NB-CONDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Deep Learning in Computer Vision</h1>\n",
    "<br>\n",
    "<a href=\"http://home.cse.shirazu.ac.ir/~m.valipour/\">Mojtaba Valipour</a> @ Shiraz University\n",
    "<h5>Workshop: 24 October, 2017: 19th Artificial Intelligence and Signal Processing Conference (AISP2017)</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div style=\"width: 200px; float:right;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "![tfImage](elements/images/kerasLogo.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"elements/images/phd.jpg\" alt=\"PHD Comic\" align=\"middle\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Download Contents of Workshop Here:\n",
    "https://github.com/mvpcom/ShirazuDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    * Tensorflow\n",
    "    * Theano (R.I.P)\n",
    "    * Keras\n",
    "    * Caffe\n",
    "    * Torch\n",
    "    * Deeplearning4j\n",
    "    * MXNet\n",
    "    * Microsoft Cognitive Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is Keras? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keras is a high-level neural networks API, for fast experimentation, written in Python and capable of running on top of TensorFlow, CNTK, or Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Install Anaconda 4.4.0 (https://repo.continuum.io/archive/)\n",
    "* pip install tensorflow==1.2.1\n",
    "* pip install keras==2.0.6\n",
    "* conda install -c conda-forge opencv\n",
    "* To solve some installation error: conda install --force html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intro to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[//]: <> (REF: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/example_network.svg)\n",
    "<img src=\"elements/images/applications/example_network.svg\" alt=\"AE\" align=\"right\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dense\n",
    "* Activation\n",
    "* Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://www.learnopencv.com/wp-content/uploads/2017/09/keras-workflow.jpg)\n",
    "<img src=\"elements/images/applications/keras-workflow.jpg\" alt=\"AE\" align=\"middle\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28) (60000,)\n",
      "Testing data shape :  (10000, 28, 28) (10000,)\n",
      "Total number of outputs :  10\n",
      "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c54f250f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c54f250f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "print('Training data shape : ', train_images.shape, train_labels.shape)\n",
    "print('Testing data shape : ', test_images.shape, test_labels.shape)\n",
    " \n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_labels)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n",
    " \n",
    "plt.figure(figsize=[10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x20c4c1aca58>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQdqTiF6pqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c54f07940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_images[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_labels[0]))\n",
    " \n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_images[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Change from matrix to array of dimension 28x28 to array of dimention 784\n",
    "dimData = np.prod(train_images.shape[1:])\n",
    "train_data = train_images.reshape(train_images.shape[0], dimData)\n",
    "test_data = test_images.reshape(test_images.shape[0], dimData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Change to float datatype\n",
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    " \n",
    "# Scale the data to lie between 0 to 1\n",
    "train_data /= 255\n",
    "test_data /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label 0 :  5\n",
      "After conversion to categorical ( one-hot ) :  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Change the labels from integer to categorical data\n",
    "train_labels_one_hot = to_categorical(train_labels)\n",
    "test_labels_one_hot = to_categorical(test_labels)\n",
    " \n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label 0 : ', train_labels[0])\n",
    "print('After conversion to categorical ( one-hot ) : ', train_labels_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Create a Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(dimData,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(nClasses, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 20s - loss: 0.2716 - acc: 0.9166 - val_loss: 0.1113 - val_acc: 0.9650\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 20s - loss: 0.0912 - acc: 0.9718 - val_loss: 0.0771 - val_acc: 0.9752\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 21s - loss: 0.0579 - acc: 0.9816 - val_loss: 0.0734 - val_acc: 0.9783\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 20s - loss: 0.0402 - acc: 0.9875 - val_loss: 0.0833 - val_acc: 0.9761\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 21s - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0916 - val_acc: 0.9740\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=5, verbose=1, \n",
    "                   validation_data=(test_data, test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9920/10000 [============================>.] - ETA: 0sEvaluation result on Test Data : Loss = 0.09164393389469479, accuracy = 0.974\n"
     ]
    }
   ],
   "source": [
    "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.16692034e-09,   3.66302905e-10,   7.04322147e-08,\n",
       "          6.12229272e-08,   2.00628319e-13,   7.51009671e-11,\n",
       "          1.56821088e-15,   9.99999523e-01,   5.44588652e-10,\n",
       "          3.59636005e-07]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data[[0],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x20c63152ba8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAGKCAYAAAB6u/nZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4VFX+x/H3SUIKEEILvYuAgggaBRVRwd5lcUVFRVDA\nlWZ317q/ddcVERVpooiICuIq4qorAlYWUcoiCKggRaoQegmBkPP740wyk0YSmMydzHxezzNPzrll\n5juh3O899xRjrUVERESiS4zXAYiIiEjoKQEQERGJQkoAREREopASABERkSikBEBERCQKKQEQERGJ\nQkoARMKYMaaXMcYaY5p7HcvRGGMaGWNGGmNWGmMOGmP2GWPmG2P+YoxJ8To+ESkozusARKR8M8Z0\nBj4EtgIjgB+BCkBH4G4gFbjHswBFpFBKAETkmBljqgH/AlYAF1pr9wfs/swY8xxwdhA+xwAVrLWH\njve9RMTRIwCRCGCM6WmM+cHX/J5ujJlkjKmb75ibjDH/8zXP7zHGLDXG9AvYf4YxZqYxZrsxJsMY\ns9oYM7qYj74Dd4c/MN/FHwBr7X5r7Uzf+5/ve5xxfr64ch5zNAnYttYY86Yxprcx5ifgEHCdMWaH\nMWZ4Id//j773aB+w7TxjzGxjzF5jzH5jzAxjTJt8511ijJlrjNnt+738bIx5vJjvLBIRlACIlHPG\nmL7AJNxdeDfgYeAS4CtjTGXfMZ2AN4GvgGuB7sArQFXf/srADOAI0Au4DPg/im8lvAjYbK1dENQv\n5VwA3Av8FbgUWABMBW40xsTmO/YW4Edr7f8AjDFXALOBfUBP4CYgGfjGGNPQd0wz3KOLNcANwNXA\ncKBSGXwXkbCjRwAi5ZjvQvg34EtrbY+A7T8B3wC9cc/lOwK7rLVDAk7/LKDcCqgGPGitXRKw/fVi\nQmgIrDvmL3B01YDTrbVbcjYYYyYB/YALcQkLxphUXILwSMC5LwJfWWuvCTj3C2A1cB8wBDgNiAfu\nstbu8R32eRl9F5GwoxYAkfKtJVALeCtwo7V2Du7CfJ5v03ygmq9Z/UpjTNV877MS2AW87Huc0LCM\n4y6JeYEXfwBr7X+BX3F3/Dl64P4vewvAGHMicALwljEmLucFHAC+BTr7zlsMHAamGGO6G2Nqlem3\nEQkzSgBEyrfqvp+bC9m3JWe/tfYr4HrcHfs0YJsxZpYxpq1v/25ck/smYDTwmzHmR2PMH4r5/PVA\n4+P+FoUr7DuBe5RxrTEmp6n+FuBza+1GXz3nQj4ed4EPfF0J1ACw1q7CPSqJwT1C2WKMmWeMyUma\nRCKaEgCR8m2H72edQvbVCdiPtfZf1trzcE3r1wF1gU+NMTG+/YuttX/AJQ1n4e60p+bvOJfPLKCu\nMeb0EsR60PczPt/2GkUcX9Ra5ZNwz+m7GWNaAGf4tuXY7vv5Z9++/K+rcj/A2i+stZfi+kJcCGQB\nHxtjapbg+4iUa0oARMq3n4Hfcc3guYwxZ+PuzL/Mf4K1dp+19iPgZVwSUCPf/ixr7TzgMdz/EScd\n5fNfBdKBkQF35IFxVDTGXOir5vQVyJ9QXHGU9y/AWvsrMBd3538LsB94P+CQn4G1QGtr7YJCXksK\nec9Ma+3nwFBcctG0NDGJlEfqBChSPlxqjNmSb9tua+1M37C1l40xb+Kax+sDf8c9138NwBjzf0Bt\n4AtcM38DYBCw2Fq7zRhzJdAX+ADXK76Sb/9e3HPzQllrd/geE3wILDLGvIR/IqAzgf64eQJmWWs3\nG2O+Av5sjEnHTRzUE2h2DL+PScAo4BRgmrV2X0BM1hhzNzDdGBOPGzmQ7vv+ZwO/WWuHG2P64/oD\nfIJ7lFET12qwyfcdRCKbtVYvvfQK0xduSJ4t4vVjwHE9gR+ATFwT+CSgbsD+K3C95jf7jlmPe0Ze\nz7e/JfAO7uJ/ENiGuzB2KGGcjYGRuMcGmbjhd/NxQxKrBBzXAPg3rsPhFuAfuLkELNAk4Li1wJtH\n+bxqvs+xwMVFHHMW8BGw0/ed1gJTgLMC9k/3/S4yfb+bd4GWXv+566VXKF7G2qIes4mIiEikUh8A\nERGRKKQEQEREJAopARAREYlCSgBERESikBIAERGRKBTx8wDUrFnTNmnSxOswREREQmLhwoXp1trU\n4o6L+ASgSZMmLFhQFiuVioiIhB9jTIlW6NQjABERkSikBEBERCQKKQEQERGJQkoAREREopASABER\nkSikBEBERCQKKQEQERGJQhE/D4CIlC8HDx5k27ZtHDx4kKysLK/DEfFcXFwciYmJpKamkpiYGLz3\nDdo7iYgcp927d/P777+TmppKnTp1iIuLwxjjdVginrHWkpWVxb59+/jtt9+oXbs2KSkpQXlvJQAi\nEjbS09Np0KABFStW9DoUkbBgjKFChQpUq1aNhIQEtmzZErQEQH0ASmPHDvjoI6+jEIlYhw4dIikp\nyeswRMJSUlISmZmZQXs/JQAlcegQ3HMPNGoEf/gDbNrkdUQiEUtN/iKFC/a/DSUAJVGhAnz3Hezf\n75KBESO8jkhEROS4KAEoCWPgwQf99bFjYc8e7+IRERE5TkoASurqq6FFC1fevRteecXbeEREjsHD\nDz+MMYYtW7Yc0/kHDx7EGEP//v2DHFnpjB07FmMM8+bN8zSO8kwJQEnFxMD99/vrzz/vHgeIiJSS\nMabEr7Vr13odrkQoDQMsjVtugcceg99/h40bYcoUuPVWr6MSkXJm0qRJeerffPMN48aNo2/fvpx7\n7rl59qWmpgb1s5966imefPLJY55QJjExkYyMDOLidPko7/QnWBqJiTBoEDzyiKsPHeqSAvVaFpFS\n6NmzZ556VlYW48aN46yzziqwryjWWg4cOEClSpVK9dlxcXHHffEO5mx04h09Aiitu+6CnH9wy5bB\nf/7jbTwiEvE+/fRTjDFMnjyZF198kVatWpGQkMBLL70EwNy5c7n11ls58cQTqVixIlWqVKFz5858\nVMi8JYX1AcjZtmbNGh544AHq169PYmIip512GjNnzsxzfmF9AAK3ff3113Tq1ImKFSuSmppK//79\nOXDgQIE4Zs2aRYcOHUhMTKRu3brcf//9LF68GGMM//znP4/5d/X777/Tv39/GjRoQHx8PI0bN2bw\n4MHs3Lkzz3H79+/n0UcfpUWLFiQlJVGtWjXatm3LIzk3eD7Tp0+nU6dO1KhRg6SkJBo3bkz37t1Z\nvXr1MccYLtQCUFrVqsGdd8ILL7j6s8/C5Zd7G5OIRIVnnnmG3bt307t3b2rVqkWzZs0AePfdd1m9\nejU9evSgUaNGbNu2jddff52rrrqK9957j27dupXo/W+88UaSkpJ48MEHycjI4Pnnn+fqq69m1apV\n1K9fv9jzv//+e959913uuOMOevbsyezZs3n55ZeJj49nRMDw6dmzZ3PZZZdRq1Yt/vKXv5CcnMyU\nKVP46quvju0X47Njxw7OOuss1q1bx5133smpp57K999/z0svvcQXX3zBvHnzcmeZ7Nu3L5MnT6ZX\nr1507NiRQ4cOsXLlSj7//PPc9/vss8+47rrraN++PY888ggpKSls2LCBmTNnsnbt2tzff7llrY3o\n1+mnn26Dbt06a2NjrQX3+u674H+GSBRavny51yF4YsKECRawEyZMKHT/f/7zHwvY1NRUu3379gL7\n9+3bV2Db3r17bdOmTW379u3zbH/ooYcsYDdv3lxgW7du3Wx2dnbu9q+//toC9sknn8zdlpGRYQHb\nr1+/AttiY2PtokWL8nxely5dbEJCgj148GDutrZt29qKFSva3377LXdbZmamPf300y1gn3766UJ/\nD4HGjBljAfvtt9/mbrv33nstYMePH5/n2GHDhlnAPvXUU9Zaa7Ozs22lSpXsddddd9TPuOuuu6wx\nxu7atavYeEKlJP9GgAW2BNdHPQI4Fo0awY03+uvPPutdLCLRwJjwfYVQ7969qV69eoHtgf0ADhw4\nwPbt2zl48CDnnXceixcvLvH0sUOGDMkz21ynTp2Ij49n5cqVJTr/vPPOo3379nm2denShczMTNav\nXw/AunXrWLJkCd27d6dhw4a5x8XHxzNo0KASfU5Rpk2bRv369enVq1ee7QMGDCAlJYVp06YBbhRG\ncnIyS5YsYcWKFUW+X0pKCtZa3nvvPY4cOXJcsYUjJQDHKnBI4Pvvw6pV3sUiIlGhRc5cJPls3ryZ\n3r17k5qaSqVKlahZsyapqam8/vrrWGvZvXt3id4/f5O2MYZq1aqxffv2YzofoEaNGgC577FmzRoA\nWrZsWeDYwraVlLWWdevWcdJJJxETk/fSlpCQQPPmzfM8tx8xYgRbtmzh5JNP5sQTT6Rv37589NFH\nuBtoZ8iQIZxyyin06dOHGjVqcNVVVzFq1KgS/z7CnRKAY3XqqXDJJa6cnQ3Dh3sbj4hEvMJWSTxy\n5Ahdu3Zl8uTJ9OnTh6lTpzJjxgxmzpxJ9+7dAcjOzi7R+8fGxha6PfCieCznl+Y9QuX6669n7dq1\nTJw4kc6dOzNjxgyuuuoqLrroIrKysgCoXbs2ixYtYtasWdx1113s2LGDQYMG0aJFCxYuXOjxNzh+\nSgCOR+D0wBMmwNat3sUiEsn8PW7C7+WxBQsWsGLFCh5//HH++c9/cv3113PxxRdz4YUXcvjwYa/D\nK6BJkyYA/PzzzwX2FbatpIwxNGnShJ9++qlAwnPo0CFWrVpVoIWiZs2a3HrrrYwfP561a9cyePBg\nZs+ezX8CRnfFxcXRtWtXnn76af773//y/fffs3PnTv7xj38cc6zhQgnA8bjgAjjtNFc+eBBGjfI2\nHhGJOjl33fnvsBctWsTHH3/sRUhH1aRJE9q0acO//vWv3H4B4C7SI45zobVrr72WDRs28MYbb+TZ\nPmrUKHbv3s11110HwOHDh9mTbz0XYwzt2rUD3GgCgPT09AKfcfLJJ5OQkJB7THmmYYDHI2eRoB49\nXH3kSFcv5cQcIiLHqm3btrRo0YKnnnqKXbt2ceKJJ7JixQpeeeUV2rZty6JFi7wOsYDhw4dz2WWX\n0bFjR/r3709ycjKTJ0/O3X+sy94+8sgjvP/++9xxxx189913tG3blgULFjBhwgTatGnDPffcA7j+\nCM2aNePaa6/l1FNPJTU1lV9//ZUxY8ZQs2ZNLvcN7b7lllvYuXMnF154IY0bN2b//v28/fbbHDx4\nkFsjYBZYJQDH6w9/gKZNYc0a2LEDXnsNBg70OioRiRLx8fF88sknPPDAA7z22mtkZGRwyimnMHny\nZObMmROWCcBFF13EJ598wiOPPMLf//53qlatSo8ePejWrRvnnXceSUlJx/S+1atX59tvv+WJJ55g\n+vTpjB8/ntq1azNgwAD++te/5vahSElJYeDAgcyePZtPP/2UAwcOULduXf7whz/w5z//OXf65dtv\nv5033niDCRMmkJ6eTkpKCm3atGH69OlcffXVQft9eMWEW8eMYEtLS7MLFiwo2w8ZOdJ/0W/SBFau\nBM2TLVJqK1as4KSTTvI6DPHIW2+9Rc+ePZk2bRrXXnut1+GEpZL8GzHGLLTWphX3XuoDEAy33w6+\noS6sXQv/+pen4YiIhLPs7GwO5VtNNTMzkxdeeIGEhAQ6d+7sUWTRRbepwVCpEgwYAH/9q6s/+yzc\ncIMWCRIRKcSePXs46aSTuPnmm2nRogXbtm1j8uTJLFu2jCeeeKLQyY4k+JQABMvdd8Mzz7jRAIsW\nweefQ9euXkclIhJ2kpKSuPjii3n//fdzFyVq1aoV48aN48477/Q4uuihBCBYUlOhd28YPdrVhw5V\nAiAiUoiEhAQmTpzodRhRT30AguneeyFnCsrPPoMffvA2HhERkSIoAQimE05wwwJzaJEgEREJUyFP\nAIwxlxpjfjbGrDLGPFzI/puNMUuMMUuNMXONMacG7Fvr277YGFPGY/uO0QMP+MtTpsC6dd7FIiIi\nUoSQJgDGmFhgFHAZcDJwozHm5HyHrQHOs9aeAvwNGJdv/wXW2nYlGePoiTPOcFMEAxw5Ai+84G08\nIiIihQh1C8CZwCpr7Wpr7SFgCnBN4AHW2rnW2p2+6jygQYhjPH6BrQCvvOJmCBQREQkjoU4A6gPr\nA+obfNuK0gf4T0DdArOMMQuNMX3LIL7guPRSaNPGlffvhzFjvI1HREQkn7DtBGiMuQCXADwUsLmT\ntbYd7hHC3caYQqeLMsb0NcYsMMYs2LZtWwiiLRBA3laAESPc/AAiIiJhItQJwEagYUC9gW9bHsaY\ntsCrwDXW2u052621G30/twLTcI8UCrDWjrPWpllr03IWdQi5Hj2gge/pxdatkG95ShERES+FOgGY\nD5xojGlqjIkHegAfBh5gjGkEvA/cYq39JWB7JWNMck4ZuBj4MWSRl1Z8PPiWngRg2DDXKVBEJAQ6\ndepE8+bN82zr2bMncSVcqGzVqlUYY3jqqaeCHltWVhbGGO64446gv7eUXEgTAGttFjAAmAGsAKZa\na5cZY/obY/r7DnscqAGMzjfcrzYwxxjzA/A98LG19tNQxl9qd94JKSmuvHIlfPjh0Y8Xkahw/fXX\nY4xh8eLFRR5jraVp06ZUrVqVjIyMEEYXHDt27ODJJ5/k66+/9jqUInXq1ImqVat6HYZnQt4HwFr7\nibW2hbX2BGvt333bxlprx/rKd1hrq/mG+uUO9/ONHDjV92qdc25YS06Gu+7y1595BiJ8+WURKV6f\nPn0AmDBhQpHHfPHFF6xdu5YePXqQlJQUlM+dMGEC+/fvD8p7FWfHjh389a9/LTQBiIuLIyMjg7Fj\nx4YkFilc2HYCjBiDBrnHAQDffQdz5ngbj4h47uKLL6Zhw4a89dZbBZbFzZGTHOQkC8FQoUIFEhIS\ngvZ+xyMxMbHEjyOkbCgBKGt168Itt/jrmh5YJOrFxMTQq1cvtm/fzoeFPBrcs2cP7733Hm3atOGM\nM87I3f72229z1VVX0ahRIxISEkhNTaVbt278+GPJukMV1Qfg66+/5uyzzyYpKYk6deowaNCgQlsK\nsrKyeOqppzj33HOpXbs28fHxNG7cmLvvvpsdAfOdzJo1ixNPPBGAxx57DGMMxpjcPglH6wPw8ssv\n0759e5KSkqhatSqXXHIJc+fOLRBHzvlz5szh3HPPpWLFitSsWZO+ffuWSSvHe++9x1lnnUWlSpWo\nXLky5557Lh999FGB4+bMmcOll15K7dq1SUhIoH79+lxxxRV8//33ucds376dwYMH06xZMxITE6lR\nowZpaWkMHz486HEfjRKAULj/fn/53/+G5cu9i0VEwsLtt9+OMabQxwBTpkwhIyOjwN3/yJEjiYuL\no1+/fowaNYo+ffrw5ZdfcvbZZ/Prr78eUxxz587loosu4tdff+Xhhx/moYceYt68edx+++0Fjj14\n8CDPPfccLVu25MEHH2TEiBF07dqVcePG0aVLFw4fPgxAmzZtGDZsGADdu3dn0qRJTJo0ieeee+6o\nsdx3333079+fxMREnn76ae655x6WLl3K+eefz2effVbg+IULF3LNNddw1lln8fzzz9O1a1deeeUV\nHggchh0EL730Et27d2f37t088cQTPProo2zdupWrrrqK1157Lfe45cuXc9FFF7Fq1SqGDBnCmDFj\nuPvuu8nOzmbp0qW5x3Xr1o0xY8Zw5ZVXMnLkSB5//HHS0tL48ssvgxp3say1Ef06/fTTbVi45hpr\nXQ8Aa2+/3etoRMLS8uXLvQ4hpLp06WJjY2Ptpk2b8mzv2LGjjY+Pt9u2bcuzfd++fQXeY+nSpbZC\nhQp24MCBebafc8459oQTTsiz7eabb7axsbF5tp1xxhk2Pj7erly5MnfbwYMH7WmnnWYB+7e//S13\n+5EjR+yBAwcKxDB27FgL2Pfeey9328qVKwucn+Pw4cMWsH369MndtmzZMgvYzp0720OHDuVuX79+\nvU1OTrbNmjWzR44cyXN+TEyMnT9/fp73vvjii218fHyhceZ3zjnn2JSUlKMek56ebpOSkmyLFi3s\nnj17crfv2rXLNm7c2FapUsXu3r3bWmvtc889ZwG7cOHCIt9v+/btFijw51VSJfk3AiywJbg+qgUg\nVAIz0jffhE2bvItFpJwxJnxfx6NPnz4cOXKENwLmCfnpp5+YN28eV199NTVr1sxzfKVKlQB347Zn\nzx7S09OpU6cOzZs357vvviv152/atIn58+fTrVu3PEMGExISGDJkSIHjY2JicjskHjlyhF27dpGe\nnk6XLl0AjimGHB988AEADz30EBUqVMjd3qBBA2677TZWr17NkiVL8pzTqVMn0tLyLgvTpUsXDh06\nxLogLcQ2Y8YMMjIyGDx4MMnJybnbU1JSGDhwIHv27OHzzz/P3ZbzXQ4WMflbxYoVqVChAvPmzQta\njMdKCUConHMOnH22Kx8+DC++6G08IuK5bt26UbVq1TyPAXKalHv37l3g+IULF3L55ZeTnJxMSkoK\nqamppKamsmLFCnbu3Fng+OKsXr0agFatWhXYd/LJ+ddpc6ZMmcIZZ5xBUlIS1apVIzU1lRYtWgAc\nUww51qxZA0Dr1q0L7MvZlhNvjmbNmhU4tkaNGoB7zh4MpYnr5ptv5oILLuBvf/sb1atXp2vXrgwd\nOpT16/0z4CcmJjJ8+HAWL15MkyZNaNOmDYMGDeKLL74ISryloQQglB580F8eOxb27PEuFhHxXGJi\nIjfddBM///wzc+fO5ciRI0yaNIkGDRpwySWX5Dl27dq1dO7cmaVLl/L4448zbdo0PvvsM2bOnEmr\nVq3Izs4u83inTp3KjTfeSFxcHCNGjODf//43M2fO5OOPPwYISQyBYmNji9xnPRhynZiYyOeff868\nefN4+OGHMcbw6KOP0rJlyzydPQcMGMCaNWt4+eWXadeuHVOnTqVLly707NkzpPFqDEYoXXUVtGwJ\nP//sLv7jxuXtICgihYrk6TP69OnD6NGjmTBhAjt27GDLli088sgjxMTkvT977733OHDgAJ9++inn\nnntu7nZrLenp6bnNz6WRcwf9008/Fdi3vJDOypMmTaJixYp88cUXJCYm5m4vbBSCKeXzkZxYli1b\nRuPGjQuNpbA7/rIWGNd5551Xorg6dOhAhw4dAFi3bh3t2rXjscce4+qrr849pn79+vTt25e+ffuS\nlZXFzTffzFtvvcV9991H+/bty/Ir5VILQCjFxOS94L/wAhQxBlhEosNpp51Gu3bteOeddxg1ahTG\nmEKb/3PudvPf2Y4dO5b09PRj+ux69eqRlpbGtGnT8owiyMzM5IUXXig0hpiYmDx3+tbaQqcLrly5\nMkCe4YFHc801bmX4Z599lqysrNztGzduZOLEiTRr1oy2bduW7IsF0cUXX0xSUhIjRozIM7xwz549\njBw5kipVqtC1a1eAQv8cGjVqRM2aNXN/DwcOHCgws2NcXBynnHIKUPLfVzCoBSDUevaExx6DLVtg\n40aYPBluu83rqETEQ3369GHgwIF8+umnnH/++YXe6V5xxRX85S9/4eabb+buu+8mJSWFOXPmMGPG\nDJo2bXrMnz18+HC6du3K2WefzZ/+9CdSUlJ4++23C21C7969O9OnT6dLly7ccsstZGZmMm3atEI7\nvNWuXZsmTZrw1ltv0aRJE2rVqkVycjJXXHFFoXGcfPLJ3HvvvQwfPpzzzjuPP/7xj+zZs4exY8eS\nkZHB6NGjC7SKBENmZmaR6x10796dVq1a8c9//pPBgwfToUMHbrvtNrKzs3n99ddZs2YN48ePz+0c\n+OSTT/LFF19w5ZVX0rRpU7Kzs5k+fTqrVq3iL3/5C+BaDS688EKuu+46WrduTbVq1Vi+fDljxozh\nhBNO4Jxzzgn6dyxSSYYKlOdX2AwDDPSPf/iHBLZuba1vaItItIu2YYA5duzYYRMTEy1g33jjjSKP\n++KLL+zZZ59tK1eubKtWrWqvuOIKu2zZskKH/JV0GGDO+3bs2NEmJCTYWrVq2QEDBtjFixcXOoxv\nzJgxtlWrVjYhIcHWrVvX9uvXz27durXAsD5rrf3222/tWWedZStWrGiB3HgKGwaYY+zYsfbUU0+1\nCQkJNjk52V500UV2zpw5eY452vmvvPKKBew333xT5O8x8HcEFPl69913c4999913bceOHW1SUpKt\nWLGiPeecc+yHH36Y5/1mzZplr7/+etuoUSObmJhoq1WrZjt06GDHjx9vs7OzrbXWbt261Q4aNMi2\nbdvWpqSk2MTERNu8eXM7ZMgQu3nz5mJjDuYwQGMj+eEakJaWZhcsWFD8gaG0cyc0agT79rn6Rx9B\nEVmxSDRZsWIFJ510ktdhiIStkvwbMcYstL51dI5GfQC8UK0a9O3rr2t6YBERCTElAF4ZMgRy5uT+\n6iu3UJCIiEiIKAHwSsOGcOON/rpaAUREJISUAHgpcHrg99+HVau8i0VERKKKEgAvnXIKXHqpK1sL\nxayUJSIiEixKALwWOD3whAmwdat3sYiISNRQAuC188+H00935cxMGDnS03BEvBbpQ5NFjlWw/20o\nAfCaMXlbAUaO9M8PIBJlYmNjOXz4sNdhiISlw4cPH3UBpNJSAhAOunWDnKk/d+4E33KgItEmOTmZ\nPVolU6RQe/bsyZ12OBiUAISDuDi4915/ffhwCFgMQyRaVK9enZ07d5Kens6hQ4f0OECinrWWQ4cO\nkZ6ezs6dO6levXrQ3ltTAYeLAwfc9MDbt7v622/nnSdAJEpkZmayY8cO9u7dy5EjR7wOR8RzsbGx\nJCcnU716dRISEoo9vqRTAWs1wHBRsSIMHAhPPunqQ4dCjx6uj4BIFElISKBu3brUrVvX61BEIpoe\nAYSTu++GpCRXXrwYZs/2Nh4REYlYSgDCSc2a0Lu3vz50qHexiIhIRFMCEG7uvRdifH8sM2fC//7n\nbTwiIhKRlACEm2bNoHt3f33YMO9iERGRiKUEIBwFLhL0zjuwbp13sYiISERSAhCO0tKgSxdXPnIE\nnn/e23hERCTiKAEIV4GtAK+84p8fQEREJAiUAISrSy5xywWDmyRozBhv4xERkYiiBCBc5V8k6KWX\nICPDu3i7Y3BeAAAgAElEQVRERCSiKAEIZzfcAA0buvLWrfDGG97GIyIiEUMJQDirUAHuucdfHzbM\ndQoUERE5TkoAwt0dd0DVqq68ahVMn+5tPCIiEhGUAIS75GS46y5//ZlnIMJXcBQRkbKnBKA8GDQI\n4uNd+fvv4ZtvvI1HRETKPSUA5UGdOnDbbf76s896F4uIiEQEJQDlxX33uaGBAB99BMuWeRuPiIiU\na0oAyouWLeGaa/x1LRIkIiLHQQlAeRI4PfBbb8HGjd7FIiIi5ZoSgPLk7LPhnHNc+fBhePFFb+MR\nEZFySwlAeRM4PfDYsbB7t3exiIhIuaUEoLy58kpo1cqV9+6FceO8jUdERMolJQDlTUwM3H+/v/7C\nC5CZ6V08IiJSLikBKI969nRzAwBs2gRvv+1tPCIiUu4oASiPEhJg8GB/fdgwyM72Lh4RESl3lACU\nV/37Q+XKrrx8OXzyibfxiIhIuaIEoLyqWhX69fPXhw71LhYRESl3lACUZ4MHQ1ycK3/zDcyb5208\nIiJSbigBKM8aNoSbbvLXtUiQiIiUkBKA8i5wSOC0abBypXexiIhIuRHyBMAYc6kx5mdjzCpjzMOF\n7L/ZGLPEGLPUGDPXGHNqSc+NSqecApdd5srWwnPPeRuPiIiUCyFNAIwxscAo4DLgZOBGY8zJ+Q5b\nA5xnrT0F+BswrhTnRqfA6YFffx1+/92zUEREpHwIdQvAmcAqa+1qa+0hYApwTeAB1tq51tqdvuo8\noEFJz41a550HZ5zhypmZMHKkt/GIiEjYC3UCUB9YH1Df4NtWlD7Af0p7rjGmrzFmgTFmwbZt244j\n3HLCmLxLBY8aBfv2eRePiIiEvbDtBGiMuQCXADxU2nOtteOstWnW2rTU1NTgBxeOunWDZs1ceedO\nGD/e23hERCSshToB2Ag0DKg38G3LwxjTFngVuMZau70050at2Ni8IwKGD4fDh72LR0REwlqoE4D5\nwInGmKbGmHigB/Bh4AHGmEbA+8At1tpfSnNu1OvVC2rWdOXffoN33/U0HBERCV8hTQCstVnAAGAG\nsAKYaq1dZozpb4zp7zvscaAGMNoYs9gYs+Bo54Yy/rCXlAQDB/rrQ4e6oYEiIiL5GBvhF4i0tDS7\nYMECr8MIne3boVEjOHDA1T/7DC66yNuYREQkZIwxC621acUdF7adAOUY1agBvXv761okSERECqEE\nIBLdey/E+P5oZ82CRYu8jUdERMKOEoBI1LQpXH+9vz5smHexiIhIWFICEKkCJwaaOhXWrPEuFhER\nCTtKACLV6adD166ufOQIPP+8t/GIiEhYUQIQyQJbAcaPdyMEREREUAIQ2S6+GNq2deUDB2D0aG/j\nERGRsKEEIJIZk3ep4JdegowM7+IREZGwoQQg0v3xj9DQt4TCtm0wcaK38YiISFhQAhDpKlRw8wLk\nGDbMdQoUEZGopgQgGtxxB1Sr5sq//goffOBtPCIi4jklANGgcmW46y5//ZlntEiQiEiUUwIQLQYO\nhIQEV54/H77+2tt4RETEU0oAokWdOnDbbf76s896F4uIiHhOCUA0ue8+NzQQ4OOP4ccfvY1HREQ8\nowQgmrRoAdde669rkSARkailBCDaBE4M9PbbsGGDd7GIiIhnlABEm44doVMnVz58GF580dt4RETE\nE0oAolFgK8DLL8Pu3d7FIiIinlACEI2uuAJOOsmV9+51SYCIiEQVJQDRKCYG7r/fX3/hBcjM9C4e\nEREJOSUA0ermm6FuXVfevBneesvbeEREJKSUAESrhAQYPNhfHzYMsrO9i0dEREJKCUA069cPkpNd\necUKNzmQiIhEBSUA0axqVZcE5Bg61LtYREQkpJQARLvBgyEuzpXnzIFvv/U2HhERCQklANGuQQPX\nITCHFgkSEYkKSgAk75DADz6AX37xLhYREQkJJQACbdrA5Ze7srXw3HPexiMiImVOCYA4gdMDT5wI\nW7Z4F4uIiJQ5JQDidO4MZ57pypmZ8NJL3sYjIiJlSgmAOMbAAw/466NHw7593sUjIiJlSgmA+F13\nHZxwgivv2gWvvuptPCIiUmaUAIhfbGzeEQHPPw+HD3sXj4iIlBklAJLXbbdBaqor//YbTJ3qbTwi\nIlImlABIXklJMHCgvz50qBsaKCIiEUUJgBT0pz9BxYquvGQJzJzpbTwiIhJ0SgCkoBo1oE8ff12L\nBImIRBwlAFK4e+91nQIBZs+GRYu8jUdERIJKCYAUrkkT+OMf/XUtEiQiElGUAEjRAicGmjoV1qzx\nLhYREQmq404AjDEnG2P+YIypF4yAJIy0bw8XXujK2dkwfLi38YiISNCUKgEwxow0xowNqHcDfgDe\nBZYbY84IcnzitcBFgsaPh/R072IREZGgKW0LwGXA3ID6X4GPgFOB74EnghSXhIsLL4R27Vw5I8Ot\nESAiIuVeaROAusBaAGNMA6A18LS1dikwAlALQKTJv0jQSy/BgQPexSMiIkFR2gTgAFDZVz4P2AMs\n8NX3AclBikvCyfXXQ6NGrpyeDhMnehuPiIgct9ImAIuAu40xbYC7gZnW2mzfvqbA5mAGJ2GiQgU3\nL0COYcPgyBHv4hERkeNW2gTgEaAjruNfS+BvAfuuxfUDkEjUpw9Uq+bKq1fD++97G4+IiByXUiUA\n1tr5QCPgTKCptXZJwO5xqBNg5Kpc2a0RkOPZZ7VIkIhIOVbqeQCstfuttQuttXtythljalhrP7bW\n/hLc8CSsDBwICQmuPH8+fPWVt/GIiMgxK+08AHcaYx4IqJ9ijNkAbDXGLDDG1Al6hBI+ateGXr38\ndS0SJCJSbpW2BWAgkBFQHw7sAoYAKcD/BSkuCVf33eeGBgL85z+wdKm38YiIyDEpbQLQGPgJwBiT\nghsK+KC19iXc8/9LinsDY8ylxpifjTGrjDEPF7K/lTHmW2NMpjHm/nz71hpjlhpjFhtjFuQ/V0Lg\nxBPhuuv89WHDvItFRESOWWkTgBggZ9hfJ8ACX/rq64FaRzvZGBMLjMLNKHgycKMx5uR8h+0ABgFF\nXVkusNa2s9amlTJ2CZbA6YHffhvWr/cuFhEROSalTQBWAlf4yj2AudbanGnh6uEu3kdzJrDKWrva\nWnsImAJcE3iAtXarb7TB4VLGJqHSoQOce64rZ2XBiy96G4+IiJRaaROAYcAQY0w6cBPwUsC+C4Al\nhZ7lVx/XUpBjg29bSVlgljFmoTGmb1EHGWP6+jolLti2bVsp3l5KLLAV4OWXYdcu72IREZFSK+08\nAG/jnvs/jWuKD5wN5nfyJgRloZO1th3uEcLdxpjORcQ5zlqbZq1NS01NLeOQotTll8PJvqc3+/a5\nJEBERMqNY5kHYI619jlr7df5tj9hrf2kmNM3Ag0D6g1820r62Rt9P7cC03CPFMQLMTFwf0AfzRde\ngMxM7+IREZFSKXUCYIypaIwZYIx51xgz2/fzT8aYpBKcPh840RjT1BgTj+tH8GEJP7eSMSY5pwxc\nDPxY2vgliG66CerVc+UtW+DNN72NR0RESqy0EwHVwS0INAJIAyr6fo4EFhljah/tfGttFjAAmAGs\nAKZaa5cZY/obY/rnfIZvcqF7gUeNMRuMMVWA2sAcY8wPuDUHPrbWflqa+CXIEhJgyBB/fdgwyM4u\n+ngREQkbxpZiPndjzBu4sf7drLX/Ddh+NvAeMMNa2yvYQR6PtLQ0u2CBpgwoM7t3Q8OGsHevq0+f\nDldf7W1MIiJRzBizsCRD5Uv7COAy4M+BF38Aa+1c4FH8QwQlWqSkQP/+/rqmBxYRKRdKmwBUBjYV\nsW+Db79Em8GDoUIFV/7vf2HuXG/jERGRYpU2AfgZuKWIfT3xTRMsUaZ+fbj5Zn/92We9i0VERErk\nWCYCutEYM8sY09sYc5kx5nZjzAzcxED6nz9aBQ4JnD4dfv7Zu1hERKRYpZ0I6E2gP9AGeBX4GBgP\ntAX6+SYKkmjUujVceaUrWwvPPedtPCIiclTHMhHQONy8/62Bc30/6wNrjTHFTQUskeyBB/zliRPd\n3AAiIhKWSp0AAFhrs621K6y1//X9zAZScMmARKtzz3ULBQEcOgQjRngbj4iIFOmYEgCRQhmTtxVg\nzBj//AAiIhJWlABIcF17LTRv7sq7dsGrr3obj4iIFEoJgARXbGzeEQHPPw+HD3sXj4iIFKrYBMAY\n06wkL6BOCOKV8uDWWyFnGeb16+Gdd7yNR0RECogrwTGrgJIsGGBKeJxEuqQkGDQIHnvM1YcOdRMF\nGeNtXCIikqskCcDtZR6FRJ4//QmefhoOHIClS2HGDLj0Uq+jEhERn2ITAGvtxFAEIhGmenW44w7/\nUMBnn1UCICISRtQJUMrOPfe4ToEAn38OCxd6G4+IiORSAiBlp0kTuOEGf12LBImIhA0lAFK2AicG\nevddWL3au1hERCSXEgApW+3awUUXuXJ2Ngwf7m08IiICKAGQUHjwQX/5tdcgPd27WEREBFACIKHQ\ntatrCQDIyIBRo7yNR0RElABICBiTtxXgpZfc/AAiIuIZJQASGtdfD40bu/L27fD6656GIyIS7ZQA\nSGjExcG99/rrzz0HWVnexSMiEuWUAEjo9OnjZggENxzw/fe9jUdEJIopAZDQqVQJ7r7bXx86FKzW\njxIR8YISAAmtAQMgMdGVFy6EL7/0NBwRkWilBEBCq1Yt6NXLXx861LNQRESimRIACb377nNDAwE+\n/RSWLPE2HhGRKKQEQEKveXPo1s1fHzbMu1hERKKUEgDxRuAiQZMnw/r13sUiIhKFlACINzp0gM6d\nXTkrC154wdt4RESijBIA8U7g9MDjxsHOnd7FIiISZZQAiHcuuwxat3blfftg7Fhv4xERiSJKAMQ7\nMTFw//3++ogRcPCgd/GIiEQRJQDirZtugnr1XHnLFnjzTW/jERGJEkoAxFvx8XDPPf76sGGQne1d\nPCIiUUIJgHivb1+oUsWVf/4Z/v1vb+MREYkCSgDEe1WqQP/+/rqmBxYRKXNKACQ8DB4MFSq48ty5\n8N//ehuPiEiEUwJQQkeOwPLlXkcRwerVg549/fVnn/UuFhGRKKAEoIQ++cQNWT//fHjnHTh0yOuI\nIlDgkMDp0+Gnn7yLRUQkwikBKKHRo93Pr76CHj2gUSN49FH47Tdv44ooJ58MV13lrz/3nHexiIhE\nOCUAJZCVBcnJEBvr3/b77/D3v0PTpnDttTBjhkavBUXgIkFvvAGbN3sXi4hIBFMCUAJxcTB1Kqxb\nB08+6Z+3BtxFf/p0uPRSaNHC3bRu3+5ZqOVfp07QsaMrHzrkZgcUEZGgUwJQCvXrwxNPwNq18N57\n0LVr3v2//uoeY9evD716wXffgbVeRFqOGZN3kaAxY2DvXu/iERGJUEoAjkGFCtCtG8ya5fqpDRkC\nKSn+/ZmZMHGiu5FNS4Px4+HAAe/iLXeuvhpOPNGVd++GV17xNh4RkQikBOA4tWwJzz8Pmza5C/3p\np+fdv2gR3HGHe2wwZIg6tpdIbGzeEQHPPw+HD3sXj4hIBFICECQVK0Lv3rBgAXz/vXsEkJjo3797\nN7z4Ipx0knt08N57uqYd1a23Qq1arrxhA0yZ4m08IiIRRglAGTjjDJgwATZudJ0CmzfPu//zz6F7\nd2jc2HUq3LjRkzDDW2IiDBrkrw8dqg4VIiJBpASgDFWvDvfe69a3+ewzN1wwJuA3vnkz/PWvLhH4\nwx9g9mxd4/K46y6oVMmVf/wRPv3U23hERCKIEoAQiImBiy6CadPcCILHHoM6dfz7jxyB99+HCy+E\nVq3ghRdg507Pwg0f1avDnXf665oeWEQkaJQAhFjDhvB//+fmFHjnHTe1cKBffoF77nFDCfv0gYUL\nPQkzfAwZ4p+B6YsvYP58b+MREYkQIU8AjDGXGmN+NsasMsY8XMj+VsaYb40xmcaY+0tzbnkSHw9/\n/KO7pi1bBgMHulVxc2RkwGuvuWGEHTrA66+7bVGncWM393IOtQKIiASFsSF86GyMiQV+AS4CNgDz\ngRuttcsDjqkFNAauBXZaa4eV9NzCpKWl2QULFpTBtwm+ffvg7bfdugM//FBwf7VqcPvt0L+/f5h8\nVPjhB2jXzpVjYlwzyQkneBuTiEiYMsYstNamFXdcqFsAzgRWWWtXW2sPAVOAawIPsNZutdbOB/IP\nkiv23PKucmXo2xf+9z+YO9etjhsf79+/cycMH+6mHL7kEvjgA7dOQcQ79VS4+GJXzs52vwQRETku\noU4A6gPrA+obfNuCeq4xpq8xZoExZsG2bduOKVAvGQNnnQWTJrkh8M884xYdCvTZZ3DddW773/4W\nBWvmBE4P/NprUA7/XEVEwklEdgK01o6z1qZZa9NSU1O9Due4pKa6a9+qVfDJJ3DllS5ByLFhAzz+\nuFue+IYb4MsvI3QoYZcu0L69Kx88CKNGeRuPiEg5F+oEYCPQMKDewLetrM8t92Ji4LLL4N//htWr\n4c9/dslBjqwst2LhBRdA69YwcqSbfTBi5F8kaORI2L/fu3hERMq5UCcA84ETjTFNjTHxQA/gwxCc\nG1GaNIF//APWr3edBjt1yrt/xQo3qqB+fejXDxYv9iTM4Ove3X15cGsuT5jgaTgiIuVZSBMAa20W\nMACYAawAplprlxlj+htj+gMYY+oYYzYA9wKPGmM2GGOqFHVuKOMPNwkJcOON8M03sGSJmzivcmX/\n/v37Ydw413J+9tnw5puu9bzciotzUyvmGD48SnpBiogEX0iHAXqhPA0DDIa9e92FfvRoN3tufjVq\nuAmG+vWDZs1CH99x27/fdXjYscPVp0xxnR9ERAQI32GAUsaSk11LwJIlrmXgxhuhQgX//u3b3bo6\nzZvD5ZfDRx+5qYjLjUqVYMAAf/3ZZyO016OISNlSAhChjHF9A95+2/UV+Mc/3I1zDmvhP/+Bq65y\nc+o8/TRs3epdvKUyYIB/reWFC910iiIiUipKAKJA7dpu1MDq1W4UwWWX5R1KuG4d/OUv0KAB3HQT\nzJkT5jfVqaluSsQcQ4d6F4uISDmlPgBRavVqePllGD/ePRbIr00b+NOf3GyEycmhj69Yq1ZBy5Zu\nZkBw0wW3bettTCIi+WRnu3nLNm1yr82b/eXA+tKlbgHUYChpHwAlAFHu4EH4179cp8Fvvy24v3Jl\nuOUW16/glFNCH99RXX+9Cx5cM0efPm754JyhgiIiZSQ7G9LTi7+wb9lSsn5WS5e6G69gUALgowSg\n5BYvhjFj3CiCAwcK7j/3XJcIdOvmhiB6bv58OPPMvNuMcQsl9O3rOjjExXkTm4iUS9nZrlX0aBf1\nnAt7MEchz5jhX/LkeCkB8FECUHq7d8Mbb7hkYMWKgvtr1fIPJWzcOPTx5TFpEjz0UOGLIdSt6wK9\n444wCFREvGRt4Rf2/Bf5LVvgcP6l6I5T9epQr5571a3rL+evB47YOh5KAHyUABw7a+Grr1wi8P77\nBbPdmBi44grXKnDJJa7uicOH3XjGl192qyTl/zttjOv52LevC1itAiIRw1o3LcjR7tY3b3avQ4eC\n+9nVqhV+IQ8s16njH7QUKkoAfJQABMfmzfDqq25mwQ0bCu5v1sy1CPTuDTVrhj6+XGvWuEDHj4ff\nfy+4v359f6tAw4YF94tIWLAWdu06+kU952dmZnA/u2rVo1/Ucy7sSUnB/dxgUQLgowQguLKy3M32\nmDHuZju/hATXN+9Pf4KOHfMONwypw4fhww9dq8DMmQX3x8S4mZD69XOtA7GxoY9RJApZ6x4zFtd5\nbtOm4F/YU1KOflGvW9e9KlYM7ueGmhIAHyUAZWflShg71q3Js3Nnwf3t2rnHAzfdlHeNgpBbvRpe\neQVee63w2Y4aNHAtAn36uLKIlJq1sGfP0e/Wc17BXpMkObn4Z+x167qJRKOBEgAfJQBlLyMD3nnH\nDSWcP7/g/ipV4LbbXDJw0kmhjy/XoUMwfbprFZg9u+D+mBi48krXKnDJJWoVEMFd2PfuLf5uffPm\nwkcPHY9KldxTu6N1nKtb1+MbjDCkBMBHCUBoLVjgHg9MnuwSg/zOP98lAtdeC/HxIQ/Pb+VK11dg\nwgQ3S0d+jRq5OQV693b/y4hEoL17i7+ob9rk1uAKpooVC17YC7vIh+UkZOWAEgAfJQDe2LkTJk50\nycAvvxTcX6eOu7727etxq3tmJnzwgWsVKGxNgdhYN59Av35ukK5nQx1ESmf3bvjtNzfV97p1rrx+\nfd4L+759wf3MpKSi79QD68nJHvYPigJKAHyUAHjLWvj8c/d4YPr0gjNixcTA1Ve7VoELL/T4+vrL\nL26Yw+uvFz4/cpMm/laBOnVCHZ1IruxsN8gl58IeeJHPKe/eHbzPS0ws2Tj2KlV0YQ8HSgB8lACE\nj40bXV+8ceMKn7eneXOXCPTqFbw5sY/JwYMwbZprFfjqq4L74+Jc1tKvXxhkLRKJDh1yd+v5L+o5\n9d9+C86Y9oSE4i/q9eq53vO6sJcfSgB8lACEn5wReqNHu9aB/BIToUcPN5TwjDNCH18eP/3kMpaJ\nE91sI/k1a+ZaBW6/3a1HIFICe/YUvKgHljdvPv4VORMTXVeWRo3cRJiNG7upL+rX91/kq1XThT0S\nKQHwUQIQ3n76yQ0lfP31wpssTz/dJQI9eng8Njdn1aRx4+Cbbwruj4tzPRv79YMuXdQqEMVymucL\nu3MPZvN8tWruoh54gQ+s16qli3u0UgLgowSgfNi/H6ZMgVGj4H//K7i/alX3aKB/f7cKsKeWL/e3\nCuzaVXB/8+auVaBXL/e/sESUoprnc8rBaJ43xt2l57+oB9bVQ16KogTARwlA+WItfP+9Gz0wZUrh\nM4F17epaBa6+2uNp/TMy4N13XTLw3/8W3F+hAlx3nWsVuOAC3Y6VE4HN84XdxQejeT4hoeg798aN\nXTO9p8NkpVxTAuCjBKD82r7dDdMfOxZ+/bXg/vr13Y32HXe4uyVPr68//ugSgTfeKLx998QT3ZjH\nXr08XiwhuhXWPJ//Il9WzfOBZTXPS1lSAuCjBKD8y8520/mPGQP//rer5xcb62YDq1TJ/QwsH8+2\nhIRS/kd94ABMneqSgW+/Lbg/Ph66dXOtAuedp6tAkOU0zxf1/H39+uOfXz6web6ou3g1z4uXlAD4\nKAGILL/95q6tr75a+GJ/wRYbexyJRPpaKs3+kMqzPqDSga1UZh+V2Ucl9pPIQUzLlq5V4LbboEaN\nsv8yEcCL5vn8F3k1z0u4UwLgowQgMh065Ibqjx0Lc+cGf53vshbDESqx3yUFZj+VqlagcsNqVK5X\nhUqVzDG3XCQmlt9Ghexst1bT0YbHFdbnsrSqVTv68/fUVA3ikPJNCYCPEoDocOiQG0mwf7+b3nTf\nPn+5sG3F7c/5We4SixiXEAT7UUhS0vEnFoU1zweWy7J5PqfcqJGbrU4kkpU0AfCyD7VI0MTHu1e1\nasF938OHg5NI5N8W7HXOc2RnuwVe9u4N7vsaU7qkoWJFSE/Pe5EPdvN8YXfxDRqoeV6kpJQAiBxF\nhQpuDoKqVYP7vjmJxf79sO+7Zex78wP2f/oN+zJi2E8lX2+Byq5csyn7W5/JvkYns+9wwlGTi7JK\nLKz1f1ZZqlq16Kb5Ro1c73k1z4sEhxIAEQ/kSSy6tXavvXvdOsovvwyLFvkPTge+wj3gv+EGGNQP\nOnYstE0+K+vYWiSKa9k4ePD4v3NO83xRz9/VPC8SWuoDIBKOFixwicDkyYUvxn7KKW4oYc+ebqWW\nMpaTWJQmkci5m8+5wKt5XiQ01AnQRwmAlGt79sDbb7tkYPHigvuTktxCCf36wZlnlt8hACISNCVN\nAPQ0TSScVaniFkBYtAi++w569867KlJGhpsusWNHaN/ezZa0Z4938YpIuaEEQKQ8MMbd4Y8fD5s2\nuVWT2rbNe8wPP7hFEurWdfMjz59//N3uRSRiKQEQKW9SUtyFfvFiN91wr17uUUCOAwdconDmmW49\n5ZdfDv64QBEp95QAiJRXxrim/wkTXKvASy9BmzZ5j/nf/9wjhLp13bTDCxd6E6uIhB0lACKRoGpV\nGDAAlixxSxPfeqsbNphj/3545RVIS3OvV14p+0H9IhLWlACIRBJj4OyzYeJE2LgRXngBTjop7zEL\nF7rWgHr14K67XCuBiEQdJQAikap6dRg8GJYtg2++cXMGJCT49+/d61ZTOu00fwfDwuYcEJGIpARA\nJNIZA506waRJrlVg+HBo2TLvMfPnu5ED9erB3Xe7RwkiEtGUAIhEkxo14J57YMUK+OoruOmmvNPz\n7dkDo0fDqafCWWe5DoYHDngXr4iUGSUAItHIGOjcGd56y7UKDBsGLVrkPWbePDfxUL16MHAg/Pij\nN7GKSJlQAiAS7WrWhPvug59+gs8/dwsOVajg3797N4wc6dYfOOcc18EwI8O7eEUkKJQAiIhjDFxw\nAUyZ4loFhg6F5s3zHjN3rpt4qF4918Fw+XJPQhWR46cEQEQKSk2FBx6An3+GWbPg+ushLmD18F27\nYMQIaN0azj3XdTBUq4BIuaIEQESKFhMDXbvC1KmwYQP885/QrFneY+bMcRMP1a/vOhj+9JM3sYpI\nqWg5YBEpnexsmD3brTEwfTpkZRU8pnNnt0xxhw6u70BgnwIRKVMlXQ5YCYCIHLstW9xQwXHjYO3a\nwo9JSnKTDXXo4CYc6tABGjd2fQ5EJOiUAPgoARAJgexsmDnTtQp8+CEcOXL042vV8icDHTrAGWe4\n9QxE5LgpAfBRAiASYps2wTvvuKWKv/8e1q0r2XktW+ZtJWjbNu8kRSJSIkoAfJQAiHhsyxaXCHz/\nPXz3nfu5Z0/x5yUkQPv2/laCM890HRD16EDkqJQA+CgBEAkz2dnwyy8uGchJCH74ofDOhPnVrOkS\ngZxWgjPPdIseiUguJQA+SgBEyoGMDLcscU4rwXffwZo1JTu3eXN/K0GHDm4dg8BVD0WijBIAHyUA\nIl8aDqsAAA5aSURBVOXU1q1ulcLAloJdu4o/Lz4e2rXL25+geXM9OpCoEbYJgDHmUuBFIBZ41Vr7\nz3z7jW//5cABoJe1dpFv31pgL3AEyCrJF1QCIBIhrIWVK/O2EixeDIcPF39utWp5Rx2ceaZ7nCAS\ngcIyATDGxAK/ABcBG4D5wI3W2uUBx1wODMQlAB2AF621HXz71gJp1tr0kn6mEgCRCJaZ6ZKAwFaC\nVatKdm6zZnlbCdq3h8TEso1XJARKmgDEFXdAkJ0JrLLWrgYwxkwBrgECVxS5BnjDusxknjGmqjGm\nrrV2c4hjFZFwl5Dgv6vPsX17wVEH27cXPHf1aveaPNnV4+Jc/4HAVoIWLdx0yCIRKNQJQH1gfUB9\nA+4uv7hj6gObAQvMMsYcAV621o4rw1hFpDyqUQMuu8y9wD06WL06byvB//7nWg8CZWXBwoXuNXq0\n25aSknfUQYcObhIjkQgQ6gTgeHWy1m40xtQCZhpjfrLWfp3/IGNMX6AvQKNGjUIdo4iEE2PghBPc\n66ab3LZDh9zQw8D+BL/8UvDc3bvdDIczZ/q3NW6ct5XgtNOgYsXQfBeRIAp1ArARaBhQb+DbVqJj\nrLU5P7caY6bhHikUSAB8LQPjwPUBCFbwIhIh4uPd9MNnnAF33+227dzpH3WQkxhs21bw3HXr3Gvq\nVFePjXWzFga2ErRqpUcHEvZC3QkwDtcJsCvuoj4fuMlauyzgmCuAAfg7AY6w1p5pjKkExFhr9/rK\nM4H/s9Z+erTPVCdAETkm1roFjgJbCRYtgoMHiz83OdklF4EtBXXrlnnIIhCmnQCttVnGmAHADNww\nwNestcuMMf19+8cCn+Au/qtwwwBv951eG5jmRgkSB7xd3MVfROSYGQNNm7rXDTe4bYcPw9KleVsJ\nVqwoeO7evfD55+6Vo2HDvKMOTj8dKlUKzXcRKYQmAhIROR67d8OCBf5Wgu++g99/L/68mBho0yZv\nK8HJJ7tHCiLHISznAfCCEgARCSlrYf36vK0ECxfCgQPFn1u5MqSl5e1PUL9+2ccsEUUJgI8SABHx\nXFYWLFuWdyjismUuWShOvXp5WwnS0lwfA5EiKAHwUQIgImFp71736CCwk+GmTcWfZwy0bp23laB1\nazeRkQhKAHIpARCRcmPjxrx9CRYsgP37iz+vYkXXqTCwk2HDhloAKUopAfBRAiAi5daRI7B8ed7+\nBD/+CNnZxZ9bp07eVoK0NDezoUQ8JQA+SgBEJKLs2+fmIwhMCtavL/48Y+DEE92raVO3GFLgT/Ur\niBhKAHyUAIhIxNu8OW9fgvnzXR+D0qhZ0yUDgYlBTrlhQ/UxKEeUAPgoARCRqJOdDT/9lHfUwZIl\n7pHCsYiNdWsg5G85yCnXqKH+BmFECYCPEgAREdw8BL/84l8Gec0af3ntWrdA0rFKTi48MWjWDJo0\ngcTEYH0LKQElAD5KAEREipGd7YYgBiYFgeXNm4/v/evVKzpBqFtXCycFmRIAHyUAIiLHKSPDtRLk\nTwxyyvv2Hft7JyS4VoKiEoQqVYL1LaJGWC4GJCIi5VBSEpx0knvlZy1s3154y8GaNfDbb0fve5CZ\nCT//7F6FqVGj8MSgWTPXObFCheB8xyikBEBERI6dMW4EQc2abt6B/A4fdsMUC2s5WLMG0tOP/v7b\nt7tXYS25MTHQqFHRCULNmuqceBR6BCAiIt7Zs8clAkUlCJmZx/7elSoVPbSxSRM3g2IE0iMAEREJ\nf1WqwKmnuld+2dmwZUvhicHq1W7q5KPZvx+WLnWvwtSpU3SCUL9+xHdOVAuA/H979x5jR1nGcfz7\noxdKKdpKS7vptrQmgEGqUExTIUHUoKQQwcQLChj8wyBqglEhWpMaCEGjifESlaBg6QUJRoKkthEN\nNxMjlyJIC/ZiQ0tLsS0txVrbWvr4xzsbD3PmdKf0cM6ZM79PMumZnXdn32ef7p5n33nnHTOzatq3\nDzZuLJ5/sGHDkS+G1Gj06DRKUHR5YeZMGD++bWG0m0cAzMysv40ZA6edlra8CNi5s/WtjZs2pcc0\nt3LgQFo3Ye3a4uMTJrS+c2H69FRA9DgXAGZm1n+kdAfBiSemByHlHTwImzcXX1rYsAG2bz/8+Xft\ngpUr05Z3zDEwONi6QDjppJ6YnOgCwMzM6mfkyDTEP2NG8fE9e4oLg6HX+/a1PvehQ2mEYdMmeOih\n5uNjxzYXBpdempZb7iAXAGZmZnnjxsGsWWnLi0iTE1sVCJs3pzat7N0Lq1enbcisWS4AzMzMepqU\nljAeGIBzzmk+vn9/mpzYav7B7t3NnzNz5pvf7xwXAGZmZu107LFw6qlpK7JrV3NRMG1aZ/uICwAz\nM7POmjABzj47bV3U36scmJmZWSEXAGZmZjXkAsDMzKyGXACYmZnVkAsAMzOzGnIBYGZmVkMuAMzM\nzGrIBYCZmVkNuQAwMzOrIRcAZmZmNeQCwMzMrIZcAJiZmdWQ4nDPLO4DkrYDG9t4yonAjjaer5v6\nJZZ+iQMcS6/ql1j6JQ5wLIdzckRMGq5R3xcA7SbpiYh4T7f70Q79Eku/xAGOpVf1Syz9Egc4lnbw\nJQAzM7MacgFgZmZWQy4Ajtyt3e5AG/VLLP0SBziWXtUvsfRLHOBYjprnAJiZmdWQRwDMzMxqyAVA\nAUkXSlojab2krxccl6QfZcf/Jml2N/pZRolYzpe0W9JT2bagG/0cjqTbJW2TtKrF8SrlZLhYKpET\nAEnTJD0o6VlJqyVdW9Cm53NTMo5K5EXSGEmPSXo6i+WGgjY9nxMoHUsl8gIgaYSkv0paVnCs8zmJ\nCG8NGzAC+AfwdmA08DRweq7NPGAFIGAu8Gi3+30UsZwPLOt2X0vEch4wG1jV4nglclIylkrkJOvr\nADA7e30CsLaKPy8l46hEXrLv87js9SjgUWBu1XJyBLFUIi9ZX78C3FnU327kxCMAzeYA6yNiQ0Qc\nAO4CLsm1uQRYFMlfgPGSBjrd0RLKxFIJEfEIsPMwTaqSkzKxVEZEbI2IJ7PX/wKeA6bmmvV8bkrG\nUQnZ93lPtjsq2/KTvXo+J1A6lkqQNAhcBPyiRZOO58QFQLOpwAsN+5tp/kVQpk0vKNvPc7IhpxWS\n3tmZrrVdVXJSVuVyImkGcBbpr7RGlcrNYeKAiuQlG2p+CtgG/CEiKpuTErFANfLyA+B64FCL4x3P\niQsAexKYHhHvAn4M3Nvl/lgFcyJpHPAb4MsR8Wq3+/NGDRNHZfISEa9FxJnAIDBH0hnd7tMbVSKW\nns+LpIuBbRGxstt9aeQCoNkWYFrD/mD2sSNt0wuG7WdEvDo0xBYRy4FRkiZ2rottU5WcDKtqOZE0\nivSmuTQi7iloUoncDBdH1fICEBGvAA8CF+YOVSInjVrFUpG8nAt8RNLzpEuxH5C0JNem4zlxAdDs\nceAUSTMljQYuA+7LtbkP+Ew2a3MusDsitna6oyUMG4ukKZKUvZ5D+j/xcsd7evSqkpNhVSknWT9v\nA56LiO+3aNbzuSkTR1XyImmSpPHZ6+OAC4C/55r1fE6gXCxVyEtEfCMiBiNiBun38AMRcUWuWcdz\nMvLNPHkVRcRBSV8Cfk+aRX97RKyW9Pns+C3ActKMzfXAXuCz3erv4ZSM5WPANZIOAv8BLotsSmov\nkfQr0mzfiZI2A98iTQiqVE6gVCyVyEnmXOBK4JnsOi3AfGA6VCo3ZeKoSl4GgDskjSC9Gd4dEcuq\n+DuMcrFUJS9Nup0TrwRoZmZWQ74EYGZmVkMuAMzMzGrIBYCZmVkNuQAwMzOrIRcAZmZmNeQCwKzP\nSbpKUrTYXuly3xZmt0KaWYd5HQCz+vg4aX3xRge70REz6z4XAGb18VRErO92J8ysN/gSgJkBr7tU\ncJ6keyXtkfSypJ9ky7A2th2QtEjSDkn7syex5Zc2JVuGerGkl7J2GyT9sKDdWZL+JGmvpHVDK6Q1\nHJ8i6Q5JL2bn2SppmaST2v+dMKsHjwCY1ccISfmf+UMRkX886RLgbuCnwBxgAXA8cBWApOOBh4EJ\npOVyXwCuABZLGhsRt2btZgKPkZY1XQCsIy2t+6Hc13sLcCfpcak3kpZA/ZmkNRHxYNZmMXAycF32\n9SYDHwTGvpFvhJm5ADCrk/wDYQB+B1yc+9jyiPha9vp+SQHcKOnmiFhLeoM+BXh/RDyUtVshaTJw\nk6TbIuI14AbgOODdEfFiw/nvyH29E4AvDL3ZS3oE+DDwKdLT3wDeC8yPiKUNn/frUlGbWSEXAGb1\n8VGaJwEW3QVwd27/LuAm0mjAWuA8YEvDm/+QJcAvgdOBZ0h/6S/LvfkX2dvwlz4RsV/SWrIH8WQe\nB67Lnvr2ALCqKg98MetVLgDM6mNVyUmA/2yxPzX7921A0WNKX2o4DnAizQVHkV0FH9sPjGnY/yTp\nqYnXky4VbJV0C3BTwSUMMyvBkwDNLG9yi/0t2b87gSkFnzel4TjADv5fNByViNgWEV+MiKnAO4CF\npEsMV7fj/GZ15ALAzPI+kdu/DDgEPJrtPwwMSjo31+7TwDbg2Wz/fuBiSQPt7FxErImI+aSRgzPa\neW6zOvElALP6OFPSxIKPPxERjQsCzZP0PdIb+BzS0PuiiFiXHV8IXAvcI+mbpGH+y4ELgKuzCYBk\nnzcP+LOkm4H1pBGBCyOi6ZbBViS9FfgjsJQ0kfG/wCWkuxDuL3seM3s9FwBm9dFq1vwk0nD9kCuA\nrwLXAAeAnwNDdwUQEf+W9D7gu8B3SLP41wBXRsSShnbPS5pLmkD4bWAc6TLCb4+w3/uAJ4HPkW4F\nPJR9vcsj4kjPZWYZeSKtmUFaCIg0i/8Urxho1v88B8DMzKyGXACYmZnVkC8BmJmZ1ZBHAMzMzGrI\nBYCZmVkNuQAwMzOrIRcAZmZmNeQCwMzMrIZcAJiZmdXQ/wBlxxMqFJtHmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c63e186d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[//]: <> (REF: https://www.learnopencv.com/wp-content/uploads/2017/10/loss-curve-without-reg.png)\n",
    "[//]: <> (REF: https://www.learnopencv.com/wp-content/uploads/2017/10/acc-curve-without-reg.png)\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td> <img src=\"elements/images/applications/loss-curve-without-reg.png\" alt=\"RE\" align=\"middle\" style=\"width: 400px;\"/></td>\n",
    "<td> <img src=\"elements/images/applications/acc-curve-without-reg.png\" alt=\"RE\" align=\"middle\" style=\"width: 400px;\"/> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The validation loss is increasing\n",
    "* The difference between the train and validation accuracy is very high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://www.learnopencv.com/wp-content/uploads/2017/10/loss-curve-with-reg.png)\n",
    "[//]: <> (REF: https://www.learnopencv.com/wp-content/uploads/2017/10/acc-curve-with-reg.png)\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td> <img src=\"elements/images/applications/loss-curve-with-reg.png\" alt=\"RE\" align=\"middle\" style=\"width: 400px;\"/></td>\n",
    "<td> <img src=\"elements/images/applications/acc-curve-with-reg.png\" alt=\"RE\" align=\"middle\" style=\"width: 400px;\"/> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: http://everglory99.github.io/Intro_DL_TCC/intro_dl_images/dropout1.png)\n",
    "<img src=\"elements/images/applications/dropout1.png\" alt=\"DR\" align=\"middle\" style=\"width: 1100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "keras.layers.Dropout(rate, noise_shape=None, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lp-Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://www.researchgate.net/profile/Younghak_Shin2/publication/230633149/figure/fig7/AS:300644943581190@1448690748255/Figure-9-L1-and-L2-norm-minimization.png)\n",
    "<img src=\"elements/images/applications/Figure-9-L1-and-L2-norm-minimization.png\" alt=\"LPNORM\" align=\"middle\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://www.youtube.com/watch?v=sO4ZirJh9ds)\n",
    "<img src=\"elements/images/applications/regu.png\" alt=\"LPNORM\" align=\"middle\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* kernel_regularizer: Regularizer function applied to the kernel weights matrix\n",
    "* bias_regularizer: Regularizer function applied to the bias vector\n",
    "* activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* keras.regularizers.l1(0.)\n",
    "* keras.regularizers.l2(0.)\n",
    "* keras.regularizers.l1_l2(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concepts and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://i.stack.imgur.com/SFST9.gif)\n",
    "[//]: <> (REF: https://cdn-images-1.medium.com/max/1600/1*1VJDP6qDY9-ExTuQVEOlVg.gif)\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td> <img src=\"elements/images/applications/SFST9.gif\" alt=\"CONV\" align=\"middle\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"elements/images/applications/conv.gif\" alt=\"CONV\" align=\"middle\" style=\"width: 300px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Auto-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://siavashk.github.io/assets/ae1.jpg)\n",
    "<img src=\"elements/images/applications/ae1.jpg\" alt=\"AE\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h5>Why AutoEncoders?</h5>\n",
    "    1. Many Algorithms works well only in Low Dimensional Cases\n",
    "    2. Huge efforts need to provide annotated examples\n",
    "    3. AutoEncoders are great for reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://cdn-images-1.medium.com/max/1600/1*8ixTe1VHLsmKB3AquWdxpQ.png)\n",
    "<img src=\"elements/images/applications/1-8ixTe1VHLsmKB3AquWdxpQ.png\" alt=\"AE\" align=\"middle\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Autoencoders:\n",
    "* Undercomplete AutoEncoder\n",
    "* Regularized AutoEncoder\n",
    "* Sparse AutoEncoder\n",
    "* Denosoising AutoEncoder\n",
    "* Variational AutoEncoder\n",
    "* Seq2Seq AutoEncoder\n",
    "* etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://deeplearning4j.org/img/deep_autoencoder.png)\n",
    "<img src=\"elements/images/applications/deep_autoencoder.png\" alt=\"AE\" align=\"middle\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can find more detail about implementation of autoencoders here:\n",
    "    * https://github.com/mvpcom/ShirazuDL/blob/master/July%202017/01_KerasExample_FirstDay.ipynb\n",
    "    * https://github.com/mvpcom/ShirazuDL/blob/master/July%202017/02_KerasExamples_SecondDay.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skip Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://image.slidesharecdn.com/dlsl2017d2l6advanceddeeparchitectures-170125171011/95/advanced-deep-architectures-d2l6-deep-learning-for-speech-and-language-upc-2017-17-638.jpg?cb=1485364567)\n",
    "<img src=\"elements/images/applications/advanced-deep-architectures.jpg\" alt=\"SK\" align=\"middle\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Residual Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://mblogthumb-phinf.pstatic.net/MjAxNzA0MjZfMTAg/MDAxNDkzMTc2MTk5MjY3.ZNrpsJ6UlcgaaZHR5QlLlZK0vp8Azuoyu84aH1hqD4wg.ZAJlUZiwje3HIuB1mxFnh9t5no1NcIg1_pXLIC4RWxcg.PNG.kangdonghyun/image.png?type=w800)\n",
    "<img src=\"elements/images/applications/residualBlock.png\" alt=\"RB\" align=\"middle\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, Input\n",
    "\n",
    "# input tensor for a 3-channel 256x256 image\n",
    "x = Input(shape=(256, 256, 3))\n",
    "# 3x3 conv with 3 output channels (same as input channels)\n",
    "y = Conv2D(3, (3, 3), padding='same')(x)\n",
    "# this returns x + y.\n",
    "z = keras.layers.add([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 256, 256, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 256, 256, 3)   84          input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 256, 256, 3)   0           input_3[0][0]                    \n",
      "                                                                   conv2d_3[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 84\n",
      "Trainable params: 84\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "Model(inputs=x,outputs=z).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transposed Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: http://deeplearning.net/software/theano/_images/no_padding_no_strides_transposed.gif)\n",
    "<img src=\"elements/images/applications/no_padding_no_strides_transposed.gif\" alt=\"RB\" align=\"middle\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "keras.layers.Conv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dilated Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: http://deeplearning.net/software/theano/_images/dilation.gif)\n",
    "<img src=\"elements/images/applications/dilation.gif\" alt=\"RB\" align=\"middle\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inception Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://i.ytimg.com/vi/VxhSouuSZDY/maxresdefault.jpg)\n",
    "<img src=\"elements/images/applications/maxresdefault.jpg\" alt=\"AE\" align=\"middle\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "\n",
    "input_img = Input(shape=(256, 256, 3))\n",
    "\n",
    "tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\n",
    "tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\n",
    "tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)\n",
    "\n",
    "tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)\n",
    "tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)\n",
    "\n",
    "output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: http://www.cvc.uab.es/people/joans/slides_tensorflow/tensorflow_html/layers_files/segnet.png)\n",
    "<img src=\"elements/images/applications/segnet.png\" alt=\"AE\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: http://rnd.azoft.com/wp-content/uploads_rnd/2016/11/overall-1024x256.png)\n",
    "<img src=\"elements/images/applications/overall-1024x256.png\" alt=\"AE\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fine-Tuning and Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trainable=False\n",
    "frozen_layer = Dense(32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(32,))\n",
    "layer = Dense(32)\n",
    "layer.trainable = False\n",
    "y = layer(x)\n",
    "\n",
    "frozen_model = Model(x, y)\n",
    "# in the model below, the weights of `layer` will not be updated during training\n",
    "frozen_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "layer.trainable = True\n",
    "trainable_model = Model(x, y)\n",
    "# with this model the weights of the layer will be updated during training\n",
    "# (which will also affect the above model since it uses the same layer instance)\n",
    "trainable_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "frozen_model.fit(data, labels)  # this does NOT update the weights of `layer`\n",
    "trainable_model.fit(data, labels)  # this updates the weights of `layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image Augmentation and Data Providing "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-6,\n",
    "    rotation_range=0.,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None,\n",
    "    data_format=K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Popular State-of-The-Art Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### VGG16/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "<img src=\"elements/images/applications/imagenet_vgg16.png\" alt=\"VGG\" align=\"middle\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "#model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Microsoft ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: http://book.paddlepaddle.org/03.image_classification/image/resnet.png)\n",
    "<img src=\"elements/images/applications/resnet.png\" alt=\"ResNet\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Yolo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://chaosmail.github.io/images/deep-learning/localizationVsDetection.png)\n",
    "<img src=\"elements/images/applications/localizationVsDetection.png\" alt=\"OB\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://pjreddie.com/media/image/model2.png)\n",
    "<img src=\"elements/images/applications/model2.png\" alt=\"YOLO\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://github.com/xslittlegrass/CarND-Vehicle-Detection/blob/master/output_images/mode_yolo_plot.jpg)\n",
    "<img src=\"elements/images/applications/mode_yolo_plot.jpg\" alt=\"YOLO\" align=\"middle\" style=\"width: 1200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[//]: <> (REF: https://github.com/xslittlegrass/CarND-Vehicle-Detection/blob/master/output_images/net_output.png)\n",
    "<img src=\"elements/images/applications/net_output.png\" alt=\"YOLO\" align=\"middle\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " * Image Classification *\n",
    " * Image Segmentation *\n",
    " * Object Detection\n",
    " * Image Generation\n",
    " * Scene Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Traffic Light Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, MaxPooling2D, Convolution2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3\n",
    "IMAGE_WIDTH = 224 \n",
    "IMAGE_HEIGHT = 224 \n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# keras image generator\n",
    "def get_generator(directory, train):\n",
    "    if train:\n",
    "        datagen = ImageDataGenerator(\n",
    "          rescale=1./255,\n",
    "          shear_range=0.2,\n",
    "          zoom_range=0.2,\n",
    "          horizontal_flip=True)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    return datagen.flow_from_directory(\n",
    "        directory=directory,\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=8,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotSpecialTool(data,labels,samples2Visualize=12,factors=[2,6], figsize=(20,12), grayFlag=False, thr=0.0):\n",
    "    # samples2Visualize = 12 # sample 12 random number\n",
    "    # factors = [2,6] # indicate two factors for number of samples\n",
    "    assert np.prod(np.array(factors))==samples2Visualize, \"%rx%r is not equal to %r\" % (factors[0],factors[1],samples2Visualize)\n",
    "    figure = plt.figure(figsize=figsize)\n",
    "    nLimit = data.shape[0]\n",
    "    for i in range(1,samples2Visualize+1):\n",
    "        img = figure.add_subplot(factors[0],factors[1],i)\n",
    "        # randomly sample an image from train set\n",
    "        imgID = np.random.randint(nLimit-1)\n",
    "        image = data[imgID]\n",
    "        #image = image[60:150,:]\n",
    "        if grayFlag:\n",
    "            plt.imshow(image.reshape(image.shape[0],image.shape[1]), cmap=plt.get_cmap('gray'))\n",
    "        else:\n",
    "            plt.imshow(image)\n",
    "        img.set_title(labels[imgID],fontsize=7)\n",
    "        plt.axis('off')\n",
    "#plotSpecialTool(centerImgs,labelsCSV[\"label\"],factors=[3,4],thr=0.0,grayFlag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load images\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "def loadImg(imgLoc):\n",
    "    imageLocation = imgLoc\n",
    "    image = misc.imread(imageLocation) #cv2.imread(imageLocation) # BGR\n",
    "    #b,g,r = cv2.split(image)       # get b,g,r\n",
    "    #image = cv2.merge([r,g,b])     # switch it to rgb\n",
    "\n",
    "    if (image is None):\n",
    "        print(imageLocation)\n",
    "     \n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "#numSample = 12\n",
    "#centerImgs = np.array([loadImg(imgLoc) for imgLoc in labelsCSV['filename'][0:numSample]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3922 images belonging to 2 classes.\n",
      "Found 110 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "directory = './Dataset/'\n",
    "train_generator = get_generator(directory+'train', True)\n",
    "validation_generator = get_generator(directory+'val', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 112, 112, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 37, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 37, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 37, 37, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 318,882\n",
      "Trainable params: 318,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Convolution2D(16, (3, 3), padding='same', strides=(2, 2), input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(3, 3)),\n",
    "  Dropout(0.2),\n",
    "\n",
    "  Convolution2D(32, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(3, 3)),\n",
    "  Dropout(0.2),\n",
    "\n",
    "  Convolution2D(64, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.2),\n",
    "\n",
    "  Flatten(),\n",
    "  Dense(128, activation='tanh'),\n",
    "  Dropout(0.5), # 0.3 works fine\n",
    "  Dense(NUM_CLASSES, activation='softmax'),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=Adam(lr=0.0003), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "filepath=\"./logs/-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='auto')\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./tensorboard', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# balance dataset\n",
    "#ls -al | grep ^[-] | wc -l # to count files\n",
    "# Red: 2621/3973 =  , Non_Red: 1353/3973 = \n",
    "\n",
    "# manual\n",
    "class_weight = {0:34 , 1:66}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2326/3922 [================>.............] - ETA: 782s - loss: 1.5620 - acc: 0.9905"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "model.fit_generator(\n",
    "  train_generator,\n",
    "  steps_per_epoch=train_generator.n,\n",
    "  epochs=1,\n",
    "  class_weight=class_weight,\n",
    "  validation_data=validation_generator,\n",
    "  validation_steps=validation_generator.n,\n",
    "  callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator, steps=len(validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load best weights for the model\n",
    "from keras.models import load_model\n",
    "model = load_model('./logs/-01-val_acc-0.98.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator, steps=len(validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['Non-Red', 'Red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# test the best model on the test pictures\n",
    "%matplotlib inline\n",
    "import glob\n",
    "testDir = directory+'test/'\n",
    "files = glob.glob(testDir+'*')\n",
    "imgL = []\n",
    "tlStatusL = []\n",
    "for file in files: \n",
    "    # load image\n",
    "    img = loadImg(file)\n",
    "    # predict \n",
    "    tlStatus = model.predict(img.reshape(1,img.shape[0],img.shape[1],img.shape[2]),batch_size=1)\n",
    "    # plot\n",
    "    imgL.append(img)\n",
    "    tlStatusL.append(labels[np.argmax(tlStatus)])\n",
    "    #print(labels[np.argmax(tlStatus)])\n",
    "    #plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(len(tlStatusL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plotSpecialTool(np.array(imgL[0:25]),np.array(tlStatusL[0:25]),samples2Visualize=25,factors=[5,5], figsize=(20,20),thr=0.0,grayFlag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plotSpecialTool(np.array(imgL[50:75]),np.array(tlStatusL[50:75]),samples2Visualize=25,factors=[5,5], figsize=(20,20),thr=0.0,grayFlag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright is important @2017\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "References:\n",
    "\n",
    "    * Thanks to Udacity SDCND and nVidia \n",
    "    * https://keras.io/getting-started/functional-api-guide/\n",
    "    * https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013\n",
    "    * https://keras.io/\n",
    "    * https://deeplearning4j.org/deepautoencoder\n",
    "    * https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "    * http://www.birving.com/presentations/autoencoders/index.html\n",
    "    * http://www.deeplearningbook.org/contents/autoencoders.html\n",
    "    * https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/\n",
    "    * https://keras.io/regularizers/\n",
    "    * https://www.researchgate.net/figure/230633149_fig7_Figure-9-L1-and-L2-norm-minimization\n",
    "    * https://www.youtube.com/watch?v=sO4ZirJh9ds\n",
    "    * https://www.slideshare.net/BAINIDA/deep-learning-and-image-analytics-using-python-by-dr-sanparit\n",
    "    * https://mblogthumb-phinf.pstatic.net/MjAxNzA0MjZfMTAg/MDAxNDkzMTc2MTk5MjY3.ZNrpsJ6UlcgaaZHR5QlLlZK0vp8Azuoyu84aH1hqD4wg.ZAJlUZiwje3HIuB1mxFnh9t5no1NcIg1_pXLIC4RWxcg.PNG.kangdonghyun/image.png?type=w800\n",
    "    * https://siavashk.github.io/2016/02/22/autoencoder-imagenet/\n",
    "    * https://keras.io/getting-started/faq/#how-can-i-freeze-keras-layers\n",
    "    * https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "    * https://stackoverflow.com/questions/44274701/make-predictions-using-a-tensorflow-graph-from-a-keras-model\n",
    "    * https://github.com/burgalon/deep-learning-traffic-lights/blob/master/train.py\n",
    "    * https://www.tensorflow.org\n",
    "    * https://github.com/ulmefors/CarND-Capstone/blob/master/classifiers/WholePicDL/trafficLightClassifer.ipynb\n",
    "    * http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n",
    "    * http://www.cvc.uab.es/people/joans/slides_tensorflow/tensorflow_html/layers.html\n",
    "    * https://github.com/xslittlegrass/CarND-Vehicle-Detection\n",
    "'''\n",
    "print('Copyright is important @2017')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
